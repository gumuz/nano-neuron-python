{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nano Neuron in python\n",
    "\n",
    "This is a python port over @trekhleb 's excellent project here: https://github.com/trekhleb/nano-neuron\n",
    "\n",
    "> NanoNeuron is an over-simplified version of the Neuron concept from Neural Networks. NanoNeuron is trained to convert temperature values from Celsius to Fahrenheit."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-97d4f0e4124b>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-97d4f0e4124b>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    > Let's implement our NanoNeuron model function. It implements basic linear dependency between x and y which looks like `y = w * x + b`.\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## The NanoNeuron model\n",
    "> Let's implement our NanoNeuron model function. It implements basic linear dependency between x and y which looks like `y = w * x + b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NanoNeuron:\n",
    "    def __init__(self, w: float, b: float):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "\n",
    "    def predict(self, x: float) -> float:\n",
    "        return x * self.w + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Celsius to Fahrenheit conversion\n",
    "\n",
    "> The temperature value in Celsius can be converted to Fahrenheit using the following formula: `f = 1.8 * c + 32`, where `c` is a temperature in Celsius and `f` is the calculated temperature in Fahrenheit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def celsius_to_fahrenheit(c: float) -> float:\n",
    "    w = 1.8\n",
    "    b = 32\n",
    "    f = c * w + b\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating data-sets\n",
    "\n",
    "> Before the training we need to generate training and test data-sets based on the celsius_to_fahrenheit() function. Data-sets consist of pairs of input values and correctly labeled output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_sets():\n",
    "    # x_train -> [0, 1, 2, ...],\n",
    "    # yTrain -> [32, 33.8, 35.6, ...]\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "\n",
    "    for x in range(100):\n",
    "        x_train.append(x)\n",
    "        y_train.append(celsius_to_fahrenheit(x))\n",
    "\n",
    "    # xTest -> [0.5, 1.5, 2.5, ...]\n",
    "    # yTest -> [32.9, 34.7, 36.5, ...]\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "\n",
    "    # By starting from 0.5 and using the same step of 1 as we have used for training set\n",
    "    # we make sure that test set has different data comparing to training set.\n",
    "    for x in range(100):\n",
    "        x += .5\n",
    "        x_test.append(x)\n",
    "        y_test.append(celsius_to_fahrenheit(x))\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The cost (the error) of prediction\n",
    "\n",
    "> We need to have some metric that will show us how close our model's prediction is to correct values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictionCost(y, prediction):\n",
    "  return (y - prediction) ** 2 / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forward propagation\n",
    "\n",
    "> To do forward propagation means to do a prediction for all training examples from `x_train` and `y_train` data-sets and to calculate the average cost of those predictions along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(model, x_train, y_train):\n",
    "    m = len(x_train)\n",
    "    predictions = []\n",
    "    cost = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        prediction = model.predict(x_train[i])\n",
    "        cost += predictionCost(y_train[i], prediction)\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    # We are interested in average cost.\n",
    "    cost /= m\n",
    "    return predictions, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Backward propagation\n",
    "\n",
    "> When we know how right or wrong our NanoNeuron's predictions are (based on average cost at this point) what should we do to make the predictions more precise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(predictions, x_train, y_train):\n",
    "    m = len(x_train)\n",
    "    # At the beginning we don't know in which way our parameters 'w' and 'b' need to be changed.\n",
    "    # Therefore we're setting up the changing steps for each parameters to 0.\n",
    "    dW = 0\n",
    "    dB = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        dW += (y_train[i] - predictions[i]) * x_train[i]\n",
    "        dB += y_train[i] - predictions[i]\n",
    "    \n",
    "    # We're interested in average deltas for each params.\n",
    "    dW /= m\n",
    "    dB /= m\n",
    "    return dW, dB"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training the model\n",
    "\n",
    "> Now we know how to evaluate the correctness of our model for all training set examples (forward propagation). We also know how to do small adjustments to parameters w and b of our NanoNeuron model (backward propagation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, alpha, x_train, y_train):\n",
    "    # The is the history array of how NanoNeuron learns.\n",
    "    cost_history = []\n",
    "\n",
    "    # Let's start counting epochs.\n",
    "    for epoch in range(epochs):\n",
    "        # Forward propagation.\n",
    "        predictions, cost = forward_propagation(model, x_train, y_train)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        # Backward propagation.\n",
    "        dW, dB = backward_propagation(predictions, x_train, y_train)\n",
    "\n",
    "        # Adjust our NanoNeuron parameters to increase accuracy of our model predictions.\n",
    "        model.w += alpha * dW\n",
    "        model.b += alpha * dB\n",
    "\n",
    "    return cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Putting all the pieces together\n",
    "\n",
    "> Now let's use the functions we have created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "neuron = NanoNeuron(w=random.random(), b=random.random())\n",
    "\n",
    "x_train, y_train, x_test, y_test = generate_data_sets()\n",
    "\n",
    "training_cost_history = train(\n",
    "    neuron,\n",
    "    epochs=70000,\n",
    "    alpha=0.0005,\n",
    "    x_train=x_train,\n",
    "    y_train=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Cost before the training: 3979.90125184559\nCost after the training: 2.356249216416798e-06\nw=1.8000649610588886,b=31.995691249383565\n"
    }
   ],
   "source": [
    "print('Cost before the training:', training_cost_history[0])\n",
    "print('Cost after the training:', training_cost_history[-1])\n",
    "\n",
    "print(f\"w={neuron.w},b={neuron.b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "158.00023852350577"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron.predict(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}